<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      What are noise variables in Structural Equation Model? &middot; 
    
  </title>

  
  <link rel="canonical" href="/what-are-noise-variables-in-sem/">
  

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script src="https://code.jquery.com/jquery-3.7.1.slim.min.js" integrity="sha256-kmHvs0B+OpCW5GVHUNjv9rOmY0IvSIRcf7zGUDTDQM8=" crossorigin="anonymous"></script>
  <script src="/public/js/sidenotes.js"></script>

  
</head>


  <body class="theme-base-08">

    

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <!-- <div class="masthead"> -->
        <!-- <div class="container"> -->
          <!-- <h3 class="masthead-title"> -->
            <!-- <a href="/" title="Home"></a>
            <small></small> -->
          <!-- </h3> -->
        <!-- </div> -->
      <!-- </div> -->

      <div class="container content">
        <div class="post">
  <h1 class="post-title">What are noise variables in Structural Equation Model?</h1>
  <span class="post-date">08 Nov 2020</span>
  <p>Given a causal graph \(\mathcal{G}\)<sup id="fnref:causal_graph_definition"><a href="#fn:causal_graph_definition" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> of variables \(X_1, \dotsc, X_n\), a structural equation model (SEM) represents the data-generating process of each node \(X_j\) as a function \(f_j\) of its parents \(\mathrm{PA}_j\) in \(\mathcal{G}\), and <strong>unobserved</strong> noise \(N_j\), i.e.
\[X_j := f_j(\textrm{PA}_j, N_j),\]
where noise variables $N_1, \dotsc, N_n$ are statistically jointly independent.<sup id="fnref:causal_graph_with_scm"><a href="#fn:causal_graph_with_scm" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> Each structural assignment represents the <i>causal mechanism</i> that generates child $X_j$ from its parents $\mathrm{PA}_j$. Here, we will explore two questions regarding unobserved noise variables:</p>

<h3>What do noise variables represent in reality?</h3>

<p>The formal definition suggests that a noise variable $N_j$ in SEM represents factors that <em>only</em> influence the corresponding observed variable $X_j$—but not other noises or observed variables.<sup id="fnref:pearl_on_noise"><a href="#fn:pearl_on_noise" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>  Moreover, as noise $N_j$ is unobserved, it also represents “phenomenon” due to which $X_j$ cannot be explained alone—deterministically—by the observed values of its parents $\textrm{pa}_j$. One example of such phenomenon is <a href="https://www.physics.umd.edu/courses/Phys276/Hill/Information/Notes/ErrorAnalysis.html">random error</a> that is always present in a measurement. It could also be other unmeasurable factors (e.g. mood) that influence $X_j$.</p>

<h3>Can we identify noise variables from data?</h3>

<p>In practice, the underlying SEM is seldom known. Instead, what we often have is a sample drawn from the joint distribution $P_{X_1, \dotsc, X_n}$. We then have to learn the SEM from the sample. This boils down to learning functions $f_j$ and unobserved noises $N_j$. Fortunately, learning the underlying functional relationship between variables is a well-studied problem with a wide range of solutions <a class="citation" href="#shwartz:2014:ml-theory-book">(Shalev-Shwartz &amp; Ben-David, 2014)</a>. Learning the unobserved noise, however, is a non-trivial learning problem for generic functions and noises.</p>

<p>To see this,<sup id="fnref:scm_learn_difficulty"><a href="#fn:scm_learn_difficulty" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> consider the bivariate causal graph $X \rightarrow Y$ induced by the SEM $Y := f(X, N)$,
where $N$ is statistically independent of $X$, i.e. $N \mathrel{\unicode{x2AEB}} X$. Suppose that noise $N$ attains values in the finite set $\mathcal{N} := \{1, \dotsc, \xi \}$.
For each value $n \in \mathcal{N}$ that $N$ takes, the assignment $Y := f(X, N)$ reduces to the
deterministic causal mechanism $Y := f(X, n)$. For a non-smooth function, for instance, $N$ may randomly select a function $f_n(X)$ for each value of $N$ from the set $\mathcal{F} := \{f(X, n) =: f_n(X) \mid n \in \mathcal{N}\}$.
Learning such SCM (with $\xi := |\mathcal{N}|$ functions) from finite data is difficult, specially as noise $N$ is <i>unobserved</i>.</p>

<p>This problem of learning SEMs from data for a given causal graph is also known as “SEM Identification Problem” <a class="citation" href="#spirtes:1998:linear-sem-identification">(Spirtes et al., 1998)</a>. Although SEM identification is infeasible for a generic class of SEMs as shown above, the problem becomes feasible with additional assumptions that restrict the class of SEMs. <a href="#tian:2009:linear-sem-identification">Tian (2009)</a><span style="display:none"><a class="citation" href="#tian:2009:linear-sem-identification">(Tian, 2009)</a></span>, for instance, studied identifiability criteria for a class of linear SEMs—also known as linear additive noise models (ANMs)—with the possibility of having correlated noise variables. For ANMs, a more general (but <strong>weaker</strong>) identification result is possible without restricting the class of functions.</p>

<p>Suppose that the causal graph $\mathcal{G}$ is induced by ANM. That is, the structural assignment of each node $X_j$ in $\mathcal{G}$ consists of an <em>additive</em> noise:
\[
    X_j := f_j(\mathrm{PA}_j) + N_j.
\]</p>

<p>Then, from a sample drawn from the joint distribution $P_{X_j, \mathrm{PA}_j}$, we can estimate $f_j$ using regression<sup id="fnref:regression"><a href="#fn:regression" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> $\hat{f}_j(\textrm{pa}_j) = \mathbb{E}[X_j \mid \textrm{PA}_j=\textrm{pa}_j]$. The residual $\epsilon_j = X_j - \hat{f}_j(\textrm{PA}_j)$ then gives us the (possibly mean-shifted) noise variable that is <strong>uncorrelated</strong> to parents $\textrm{PA}_j$, i.e. $\mathrm{Cov}(\epsilon_j, \textrm{PA}_j)=0$.</p>

<p>Note that zero correlation is a weaker condition than statistical independence. The latter implies the former, but not the other way around. In special cases (e.g. multivariate Gaussian distribution), however, mean independence implies statistical independence (see <a href="https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent">here</a>). Thus, for jointly Gaussian distributed noise and parents, we can recover the noise up to statistical independence.</p>

<p>We still have to learn the regression function from data. To this end, we optimise some “loss function”—that measures how well a function $f$ fits the data—over a class of functions $\mathcal{F}$. Although regression estimates of many well-known loss functions (e.g. $\ell_1$, $\ell_2$) converge to the conditional mean asymptotically as the sample size approches infinity, for a finite sample size, some loss functions yield optimal results than the others depending on the distribution of noise. Whereas $\ell_1$-loss function gives optimal result for a Laplace distributed noise, $\ell_2$ loss provides optimal result for a Gaussian noise.<sup id="fnref:ml_estimation"><a href="#fn:ml_estimation" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> If the distribution of the unobserved noise is unknown (which is often the case), it is hard to tell which loss function will yield optimal result. A promising approach then is to use a regression procedure that minimises dependence between residuals and regressors based on a non-parametric test of indpendence <a class="citation" href="#mooij:2009:dependence-minimisation-regression">(Mooij et al., 2009)</a>. The non-convexity of the resulting optimisation problem, however, makes it difficult to solve; as such, we have to rely on variants of Newton’s method that are approximate and often time-consuming.</p>

<p>Lastly, as causal inference rests on assumptions, it is crucial to check our assumptions, whenever possible. In case of ANM, for instance, we can run a hypothesis test to check if the unobserved noise we recover from data is statistically independent of regressors.</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="pearl:2012:sem">Pearl, J. (2012). <i>The Causal Foundations of Structural Equation Modeling</i>. Guilford Press, New York.</span></li>
<li><span id="spirtes:1998:linear-sem-identification">Spirtes, P., Richardson, T., Meek, C., Scheines, R., &amp; Glymour, C. (1998). Using Path Diagrams as a Structural Equation Modeling Tool. <i>Sociological Methods &amp; Research</i>, <i>27</i>(2), 182–225.</span></li>
<li><span id="tian:2009:linear-sem-identification">Tian, J. (2009). Parameter Identification in a Class of Linear Structural Equation Models. <i>IJCAI 2009, Proceedings of the 21st International Joint Conference
               on Artificial Intelligence, Pasadena, California, USA, July 11-17,
               2009</i>, 1970–1975.</span></li>
<li><span id="schölkopf:2019:causality-ml">Schölkopf, B. (2019). <i>Causality for Machine Learning</i>.</span></li>
<li><span id="shwartz:2014:ml-theory-book">Shalev-Shwartz, S., &amp; Ben-David, S. (2014). <i>Understanding Machine Learning: From Theory to Algorithms</i>. Cambridge University Press.</span></li>
<li><span id="peters:2017:book">Peters, J., Janzing, D., &amp; Schölkopf, B. (2017). <i>Elements of Causal Inference - Foundations and Learning Algorithms</i>. The MIT Press.</span></li>
<li><span id="mooij:2009:dependence-minimisation-regression">Mooij, J. M., Janzing, D., Peters, J., &amp; Schölkopf, B. (2009). Regression by dependence minimization and its application to causal inference in additive noise models. <i>Proceedings of the 26th International Conference on Machine Learning</i>, 745–752.</span></li>
<li><span id="cover:2006:information-theory-book">Cover, T. M., &amp; Thomas, J. A. (2006). <i>Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)</i>. Wiley-Interscience.</span></li></ol>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:causal_graph_definition">

      <p>A causal graph is a directed acyclic graph that represents data-generating process that operates sequentially along its arrows. <a href="#fnref:causal_graph_definition" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:causal_graph_with_scm">

      <p>A toy causal graph alongside its SEM. <img src="../assets/images/sem.png" alt="Toy SCM" /> <a href="#fnref:causal_graph_with_scm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:pearl_on_noise">

      <p>Pearl <a class="citation" href="#pearl:2012:sem">(Pearl, 2012)</a> writes, “they represent observed or unobserved background factors that the modeler decides to keep unexplained—that is, factors that influence but are not influenced by the other variables (called “endogenous”) in the model … The[y] <del>former</del> are shaped by physical reality (e.g., genetic factors, socioeconomic conditions)<del>, not by analysis</del>; they are treated as any other variable, though we often cannot measure their values precisely and must resign ourselves to merely acknowledging their existence and assessing qualitatively how they relate to other variables in the system.” <a href="#fnref:pearl_on_noise" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:scm_learn_difficulty">

      <p>The difficulty in learning unobserved noises in SEM from data for generic choices of functions and noises is also explained well in <a href="#schölkopf:2019:causality-ml">Schölkopf (2019)</a><span style="display:none"><a class="citation" href="#schölkopf:2019:causality-ml">(Schölkopf, 2019)</a></span>, and the general idea is briefly mentioned in <a href="#peters:2017:book">Peters et al. (Chapter 2, 2017)</a><span style="display:none"><a class="citation" href="#peters:2017:book">(Peters et al., 2017)</a></span> when drawing the connection between SEM and the principle of independent mechanism. <a href="#fnref:scm_learn_difficulty" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:regression">

      <p>A <strong>regression function</strong> $r:\mathcal{X} \rightarrow \mathcal{Y}$ (where $\mathcal{Y}$ is often $\mathbb{R}$) gives us the expected value of the response variable $Y$ given an instance $x$ of the regressor $X$, i.e. $r(x) = \mathbb{E}[Y \mid X=x]$. It may belong to some finite (e.g. set of straight lines) or infinite family $\mathcal{R} \ni r$. A regression model from any family $\mathcal{R}$ can always be written as
\[
    Y = r(X) + \epsilon,
\]
where $\mathbb{E}[\epsilon] = 0$. This is because we can define $\epsilon = Y - r(X)$ s.t. $Y = Y + r(X) - r(X)$ $= r(X) + \epsilon$, and mean of the residual $\epsilon$ is
<!-- <div style="display: inline-block; visibility:hidden">space more </div> --></p>
      <div style="clear:both"></div>
      <div style="margin:auto;">
$$
    \begin{align}
    \mathbb{E}[\epsilon] &amp;= \mathbb{E}[\mathbb{E}[\epsilon \mid X]]\\
                        &amp;= \mathbb{E}[\mathbb{E}[ Y - r(X) \mid X]]\\
                        &amp;= \mathbb{E}[\mathbb{E}[ Y  \mid X] - \mathbb{E}[r(X) \mid X]]\\
                        &amp;= \mathbb{E}[r(X) - r(X)]\\
                        &amp;= 0
    \end{align}
$$
</div>
      <!-- <span style="display: inline-block; visibility:hidden">more space</span> -->
      <div style="clear:both"></div>
      <p>Above, we observe that $\mathbb{E}[\epsilon \mid X] = 0$. Thus, we have $\mathbb{E}[\epsilon \mid X] = \mathbb{E}[\epsilon]$. That is, residual $\epsilon$ and input $X$ are <strong>mean independent</strong>. Mean independence also implies uncorrelatedness, $\textrm{Cov}(\epsilon, X)=0$. Note that if $\epsilon$ has a non-zero mean then we can incorporate that into $r(X)$ as an additive constant. <a href="#fnref:regression" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ml_estimation">

      <p>The $\ell_1$-loss minimisation yields median for the parameter, whereas $\ell_1$-loss yields mean. Note that the maximum likelihood estimates of the parameters of a Gaussian distribution can be computed from the sample mean. For a Laplacian distribution, they can be computed using sample median. Therefore, $\ell_2$ loss function is appropriate for a Gaussian noise, whereas $\ell_1$ loss function is suited for a Laplacian noise. <a href="#fnref:ml_estimation" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</div>



      </div>
    </div>

    <!-- <label for="sidebar-checkbox" class="sidebar-toggle"></label> -->

    <script src='/public/js/script.js'></script>
  </body>
</html>
